{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issues with loans appear to be related to errors and mismanagement by loan servicers, including:\\n\\n- Dealing with your lender or servicer (e.g., receiving bad information, errors in loan balances, misapplied payments, wrongful denials of payment plans).\\n- Incorrect or inconsistent information on credit reports and account status (e.g., falsely reported delinquencies, improper account transfers, incorrect balances).\\n- Trouble with how payments are handled, such as restrictions on applying payments toward principal or paying off loans early.\\n- Discrepancies and errors in loan balances, interest calculations, and loan transfer notifications.\\n- Challenges with loan forgiveness, cancellation, or discharge, and long-term forbearance issues.\\n- Issues related to borrower rights violations, like privacy concerns and improper data handling.\\n\\nWhile specific problems vary, errors in account information, misapplication of payments, and difficulties caused by servicer misconduct seem to be the most common themes.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, yes, some complaints were not handled in a timely manner. Specifically, at least two complaints indicate delays:\\n\\n1. Complaint from 03/28/25 (Complaint ID: 12709087) submitted to MOHELA, where the response was marked as \"No\" for timely response, indicating it was not handled promptly.\\n\\n2. Complaint from 04/24/25 (Complaint ID: 13160766) submitted to Maximus Federal Services, which was marked as \"Yes\" for timely response, so this one was handled in time.\\n\\nAdditionally, many complaints mention ongoing issues or delays in resolving their issues, such as complaints from 04/14/25 and 04/18/25 that highlight unresolved problems after extended periods.\\n\\nHowever, the complaint from 03/28/25 regarding MOHELA stands out as definitely not handled in a timely manner.\\n\\nIn summary, yes, some complaints, particularly the one from MOHELA, did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors highlighted in the complaints:\\n\\n1. **Limited or Misleading Payment Options:** Borrowers were often offered only options like forbearance or deferment, which led to continuous interest accumulation. This increased the total amount owed and extended repayment periods, making it difficult to pay off the loans.\\n\\n2. **Unawareness and Poor Communication:** Many borrowers were not adequately informed about their loan status, repayment schedules, or changes in loan servicers. For example, some were unaware that their loans had been transferred to new servicers or that their repayment was due earlier than expected.\\n\\n3. **Interest Accumulation and High Debt:** Continuous interest buildup, especially when payments are lowered or delayed, can cause the debt to grow over time despite ongoing payments. Some borrowers noted that their balances increased despite making payments, due to high interest rates and mismanagement.\\n\\n4. **Inadequate Support and Complicated Processes:** Difficulties in applying extra payments to the principal, or in understanding loan terms, serve as barriers to faster repayment. Some borrowers reported that their payments were applied primarily to interest, prolonging the debt.\\n\\n5. **Financial Hardship and Economic Conditions:** Many borrowers faced financial hardships, including stagnating wages, job loss, or high living costs, which made it impossible to increase payments or pay off the loans within the estimated timeline.\\n\\n6. **Unfavorable Loan Management:** Several complaints point to unresponsive or confusing loan servicing practices, including incorrect reporting, lack of transparency, and inadequate assistance, leading to missed payments or default.\\n\\n7. **Loan Transfer and Poor Notification:** Transfers between loan servicers often occurred without proper notification, leaving borrowers unaware of their payment obligations or delinquency status, resulting in late payments and damage to credit scores.\\n\\nIn summary, the failure to pay back loans was often due to a combination of high interest accumulation due to limited payment options, poor communication from lenders/servicers, economic hardships, and complicated loan management practices.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with student loan complaints appears to be dealing with lenders or servicers, specifically issues related to incorrect or unclear information about loans, problems with repayment procedures, and perceived bad practices like overcharging or miscommunication. Multiple complaints highlight difficulties in managing loan payments, understanding loan balances, or resolving issues with loan servicing companies.\\n\\nTherefore, the most common issue with loans, as reflected in these complaints, is problems related to \"Dealing with your lender or servicer.\"'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all of the complaints mentioned in the context received timely responses from the companies. Specifically, the complaints with IDs 13197090, 12792958, 13160766, and 13410623 are all marked as \"Timely response?\": \"Yes.\" Therefore, there is no indication that any complaints went unhandled in a timely manner.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including problems with their payment plans, miscommunication or lack of communication from the loan servicers, issues with loan transfers and automatic payments, and difficulties resolving disputes or obtaining forbearance. Some specific issues highlighted include being steered into incorrect repayment options, not receiving notices about loan status changes, technical problems with payments (such as payments being reversed or not processed), and the servicers not responding adequately to payment or deferment requests. These issues can lead to missed payments, increased debt due to interest capitalization, and negative impacts on credit scores.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "\n",
        "**Answer**\n",
        "\n",
        "\n",
        "BM25 does absolute exact matches and has low semantic ambiguity. It is best for retrieval of questions with keywords that match exactly in the context, for example questions about technical specifications, legal or medical documents eg what are the onset symptoms of <insert awful disease name>. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, including errors, mishandling, and misinformation. Specific recurring issues include incorrect or inconsistent loan balances, misapplied payments, wrongful denials of payment plans, and lack of proper communication or documentation. Additionally, complaints often involve concerns over privacy violations, unauthorized transfer of loans, and mishandling of personal information.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, it appears that at least one complaint (the one regarding a previous complaint about payments not being applied to the account) was addressed with a response marked as \"Closed with explanation\" and a note indicating the response was timely. \\n\\nHowever, other complaints, such as the one about the long delay in response to a request for account review and possible violations, indicate that the issue has not been resolved after a long period (nearly 18 months) and no resolution has been reached.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for several reasons, including:\\n\\n1. Lack of Understanding: Borrowers were often not fully informed about their repayment obligations or how interest accumulates, leading to confusion and unawareness of the true debt they owed.\\n2. Transfer and Management Issues: Loans were transferred between different servicers without borrowers' knowledge or consent, causing confusion and processing issues.\\n3. Difficulties with Payment Options: Available options like forbearance or deferment allowed interest to keep accruing, which increased the total debt over time and made repayment more difficult.\\n4. Financial Hardship: Borrowers faced financial hardship due to stagnant wages, economic conditions, or personal circumstances, making it hard to keep up with payments or manage increasing interest.\\n5. Administrative Problems: Errors such as incorrect account information, failure to notify borrowers of payments due, or miscommunication about repayment schedules contributed to missed payments.\\n6. Unmanageable Loan Terms: The combination of rising interest, unmanageable repayment plans, and lack of affordable options led many to fall behind or be unable to make payments.\\n\\nOverall, a mix of inadequate information, administrative errors, and financial hardship contributed to the failure to pay back loans.\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints and context, the most common issues with student loans appear to be:\\n\\n- Errors and inaccuracies in loan balances or account information.\\n- Problems with how payments are being applied or handled, often leading to late payments or negative impact on credit scores.\\n- Lack of proper communication, notices, or transparency from loan servicers regarding important changes or errors.\\n- Mischandling or mismanagement of loan information, including incorrect reporting to credit bureaus.\\n- Issues with loan consolidation, including lack of disclosure and unexpected payment amounts.\\n- Difficulty expressing concerns or resolving issues due to poor customer service.\\n- Problems with loan forgiveness, discharge, or handling of special cases like bankruptcy or settlement eligibility.\\n\\nOverall, a prevalent theme is mismanagement and poor communication leading to inaccuracies, unforeseen charges, or adverse credit impacts for borrowers.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, according to the provided complaints, several complaints indicate that issues were not handled in a timely manner. Examples include:\\n\\n- A complaint from 04/18/25 (Complaint ID: 13062402) about a credit report and investigation still unresolved after over a year.\\n- Multiple complaints about unresolved account issues, with delay of over a year or more in responses and resolution.\\n- Several complaints mention that the companies (such as Maximus Federal Services and Mohela) either responded with explanations that did not resolve the core issues or closed the cases without resolution, despite the consumer repeatedly following up over extended periods.\\n\\nOverall, there is evidence that some complaints did not get handled promptly, with delays extending well beyond a typical response timeframe, suggesting that they were not handled in a timely manner.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, including:\\n\\n1. Lack of clear or accurate information from lenders or servicers about repayment options, interest accrual, and consequences of forbearance or deferment.\\n2. The accumulation of interest during forbearance or deferment periods, which increased the total amount owed and extended the repayment timeline.\\n3. Unmanageable high interest rates and the negative impact of interest capitalization, making it difficult or impossible to pay off the loans.\\n4. Financial hardships, such as unemployment, stagnant wages, or other life circumstances, that made consistent repayment difficult.\\n5. Systemic issues like mismanagement, errors in loan balances, miscommunication about loan statuses, or improper reporting to credit bureaus.\\n6. Coercive or unethical servicing practices, such as forbearance steering, failure to inform borrowers of income-driven repayment options, or improper handling of loans, which contributed to borrowers falling behind or being unable to manage repayment effectively.\\n\\nOverall, many borrowers struggled not due to irresponsibility but because of systemic failures, lack of transparency, confusing options, and financial hardships.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "**Answer**\n",
        "\n",
        "Often users vague questions or questions that are not properly worded. LLMs are good at writing prompts for other LLMs. We can leverage this to better word the query, add more keywords, use synonyms, hypernyms or concepts and structured that help with context retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided context, appears to be problems related to improper or incorrect handling of loan information, such as errors in loan balances, misreporting or misinformation on credit reports, and issues stemming from the loan servicing process. Specifically, complaints include incorrect information on credit reports, errors in loan balances, misapplied payments, wrongful denials of payment plans, and issues with loan transfer and interest rate increases.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that at least two complaints were not handled in a timely manner. Specifically, the complaints involving MOHELA (Complaint IDs 12709087 and 12935889) mention that no one has reached out to the complainant despite multiple follow-up calls, and the responses were marked as \"No\" for timeliness. This indicates that these complaints were not addressed within an acceptable or timely timeframe.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including severe financial hardship, misrepresentations by educational institutions about employment prospects and the value of their degrees, and issues with loan servicing. Specifically, some borrowers experienced difficulties due to the inability to secure employment after graduation, which made it challenging to make loan payments. Others faced issues with loan repayment due to administrative problems, such as being incorrectly billed, being reported as delinquent without proper notification, or dealing with the complexities of loan ownership and servicing following the closure of schools or institutions. Additionally, some borrowers had to rely on deferment and forbearance options, which increased the overall debt due to accumulated interest.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, is dealing with the loan servicer or lender, which includes problems such as receiving bad information about the loan, trouble with how payments are being handled, errors or discrepancies in loan balances, unintended default or delinquency reporting, improper transfer of loans without proper notice, and issues with loan consolidation or adjustments. Many complaints highlight mismanagement, lack of communication, inaccurate reporting, and deceptive practices by loan servicers.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, yes, there are complaints indicating that some issues were not handled in a timely manner. For example, several complaints noted that responses from the companies were late or that the companies failed to respond within the expected timeframe. Specifically, one complaint from EdFinancial Services mentioned a response that was not timely, and others highlighted delays or lack of response despite multiple follow-ups.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including:\\n\\n1. Lack of Notification and Communication: Many borrowers were not properly notified about payment due dates, loan transfers, or changes in their account status, leading to missed payments and reporting errors on credit reports.\\n\\n2. Improper Handling and Mismanagement by Servicers: Some borrowers experienced errors in their loan balances, misapplied payments, or wrongful reporting of delinquency due to errors or lack of proper procedures by servicers like Nelnet, Maximus, or MOHELA.\\n\\n3. Difficulties with Payment Plans and Interest Accumulation: Borrowers faced challenges due to limited options beyond forbearance or deferment, interest continuing to accrue and capitalize during these periods, making loans larger and repayment unmanageable.\\n\\n4. Lack of Clear or Adequate Information: Many were misled or inadequately informed about repayment options, income-driven repayment plans, consolidation, or forgiveness programs, which led to confusion and unintentional delinquency.\\n\\n5. Economic Hardship and Unforeseen Circumstances: Factors such as unemployment, financial hardship, or lack of sufficient income prevented borrowers from making payments, even when they were willing.\\n\\n6. Errors and Delays in Processing: Some borrowers experienced delays in processing deferments, forbearances, or income-driven plans, leading to penalties and credit score drops.\\n\\n7. Exploitative Practices: Reports of forbearance steering, coercive consolidation, and lack of transparency contributed to increased debt and inability to manage repayment.\\n\\nOverall, a combination of systemic issues, miscommunication, financial hardship, and in some cases, misconduct by loan servicers contributed to people's failure to pay back their loans.\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be difficulties with loan servicing and management. This includes problems such as struggling to repay loans, improper use or reporting of loan information, issues with payment plans and auto-debit setup, miscommunication about loan status or issuer changes, and violations of privacy or legal protections related to student loans. Many complaints center around errors, lack of transparency, delays, and disputes over loan account information.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that several complaints were marked as \"Closed with explanation\" and the responses time was indicated as \"Yes\" for the complaints with response status. This suggests that the complaints were handled within the expected time frame.\\n\\nHowever, the details also highlight that multiple complaints involved:\\n- No response or inadequate responses from the companies.\\n- Issues such as unaddressed misconduct, errors, or violations of laws.\\n- Complaints where the consumer was still requesting investigations or corrections.\\n\\nWhile some complaints stated they received timely responses and were \"Closed with explanation,\" there are multiple instances where complaints involved ongoing issues or inadequate handling, which may imply delays or unresolved matters in practice. \\n\\nTo directly answer your question: **No, according to the data, all complaints marked as \"timely response\" were handled in an appropriate time frame. However, several complaints involved unresolved issues or allegations of unaddressed misconduct, indicating that not all complaints were effectively or satisfactorily handled in a timely manner.**'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the provided context, people failed to pay back their loans for various reasons, including:\\n\\n- Lack of clear or accurate information from lenders or servicers, leading to confusion and stress (e.g., receiving bad information or lack of transparency).\\n- Administrative issues or delays, such as processing payments or documentation, which may have caused missed payments or default notices.\\n- Disputes over the legitimacy or status of the loans, including claims of wrongful default, unverified debts, or legal complications.\\n- Difficulties in communication or customer service, which hindered resolution or understanding of their repayment obligations.\\n- Problems related to changes in payment plans or re-amortization, impacting affordability or payment schedules.\\n- Cases of alleged improper reporting or illegal collection practices, leading borrowers to believe their debt records were inaccurate or invalid.\\n\\nIn summary, failures to pay back loans often stemmed from procedural issues, misinformation, disputes over the debt's validity, or difficulties in communication and administrative processing.\""
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "\n",
        "**Answer**\n",
        "\n",
        "If sentences are very similar then the sematic similarity will be consistently high. This could make it difficult to find a threshold to chunkify the documents. If the threshold is too high it might lead to the creation of too many small chunks that are very close in content and will cause context repetition in the vector store. Another possible outcome is that if the threshold is too low then the algorithm will produce few very large chunks and grouping different FAQs together. This would degrade retrieval as one query might return a very large chunk of data with multiple irrelevant FAQs. We could mitigate this by adding a chunk splitting rule based on the structure of the document: eg extracting based on keywords like \"Question\" and \"Answer\" or paragraphs or sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modify chains for langsmith parsing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "naive_retrieval_chain_langsmith = (\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model | StrOutputParser()} \n",
        ")\n",
        "\n",
        "bm25_retrieval_chain_langsmith = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model | StrOutputParser() }\n",
        ")\n",
        "\n",
        "contextual_compression_retrieval_chain_langsmith = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model | StrOutputParser()}\n",
        ")\n",
        "\n",
        "multi_query_retrieval_chain_langsmith = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model | StrOutputParser()}\n",
        ")\n",
        "\n",
        "parent_document_retrieval_chain_langsmith = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model| StrOutputParser()}\n",
        ")\n",
        "\n",
        "ensemble_retrieval_chain_langsmith = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model| StrOutputParser()}\n",
        ")\n",
        "\n",
        "semantic_retrieval_chain_langsmith = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model| StrOutputParser()}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the pdf files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3dc526e874b40cc9cec3d820f791b65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "608fc5c15d10406a94ef845cfbde98ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57924beb9a394a7cbfca0aa83f93dd6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/31 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary' already exists in node 'cc2ddb'. Skipping!\n",
            "Property 'summary' already exists in node '4ae810'. Skipping!\n",
            "Property 'summary' already exists in node '3d8dd3'. Skipping!\n",
            "Property 'summary' already exists in node 'b17c47'. Skipping!\n",
            "Property 'summary' already exists in node 'f63f23'. Skipping!\n",
            "Property 'summary' already exists in node '14b391'. Skipping!\n",
            "Property 'summary' already exists in node '5c8bff'. Skipping!\n",
            "Property 'summary' already exists in node '47347a'. Skipping!\n",
            "Property 'summary' already exists in node '0df40a'. Skipping!\n",
            "Property 'summary' already exists in node '6eaece'. Skipping!\n",
            "Property 'summary' already exists in node '9b3fa7'. Skipping!\n",
            "Property 'summary' already exists in node 'cf0166'. Skipping!\n",
            "Property 'summary' already exists in node '8a8e9f'. Skipping!\n",
            "Property 'summary' already exists in node 'ef3dca'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d4fe57ddf7b402aa7ebed8bfb7adf44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "748bcd45ab3046dda06ce835643b2ae8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/41 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary_embedding' already exists in node 'b17c47'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '5c8bff'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'cc2ddb'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '6eaece'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'cf0166'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '47347a'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '8a8e9f'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'ef3dca'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '3d8dd3'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '4ae810'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '0df40a'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '14b391'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'f63f23'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '9b3fa7'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a2d25a5df914116bcfc15eeb349dec2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95e449abf5c44ca8a86953107f2a9224",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f49426bfd62a45f9a6694f41240dddd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8a34da6a4594fc5925bd0ae96e5dfca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "ragas_dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>where i find more about subscription-based pro...</td>\n",
              "      <td>[non-term (includes clock-hour calendars), or ...</td>\n",
              "      <td>For more detail on subscription-based programs...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does clinical work in nursing programs aff...</td>\n",
              "      <td>[Inclusion of Clinical Work in a Standard Term...</td>\n",
              "      <td>Clinical work in nursing programs may be inclu...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How are Title IV disbursements managed for non...</td>\n",
              "      <td>[Non-Term Characteristics A program that measu...</td>\n",
              "      <td>Title IV program disbursements, except for the...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do the requirements for clinical or practi...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nInclusion of Clinical Work in a St...</td>\n",
              "      <td>The requirements for clinical or practicum exp...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what if a program got self-paced or independen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nInclusion of Clinical Work in a St...</td>\n",
              "      <td>If a program has self-paced or independent stu...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do the disbursement requirements for feder...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nboth the credit or clock hours and...</td>\n",
              "      <td>In clock-hour or non-term credit-hour programs...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do the requirements outlined in Volume 8, ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nnon-term (includes clock-hour cale...</td>\n",
              "      <td>In subscription-based programs, the administra...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>If a medical program includes clinical work th...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nInclusion of Clinical Work in a St...</td>\n",
              "      <td>When a medical program includes clinical work ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>how direct loan program work for student in no...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nnon-term (includes clock-hour cale...</td>\n",
              "      <td>For students in non-term or subscription-based...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  where i find more about subscription-based pro...   \n",
              "1  How does clinical work in nursing programs aff...   \n",
              "2  How are Title IV disbursements managed for non...   \n",
              "3  How do the requirements for clinical or practi...   \n",
              "4  what if a program got self-paced or independen...   \n",
              "5  How do the disbursement requirements for feder...   \n",
              "6  How do the requirements outlined in Volume 8, ...   \n",
              "7  If a medical program includes clinical work th...   \n",
              "8  how direct loan program work for student in no...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [non-term (includes clock-hour calendars), or ...   \n",
              "1  [Inclusion of Clinical Work in a Standard Term...   \n",
              "2  [Non-Term Characteristics A program that measu...   \n",
              "3  [<1-hop>\\n\\nInclusion of Clinical Work in a St...   \n",
              "4  [<1-hop>\\n\\nInclusion of Clinical Work in a St...   \n",
              "5  [<1-hop>\\n\\nboth the credit or clock hours and...   \n",
              "6  [<1-hop>\\n\\nnon-term (includes clock-hour cale...   \n",
              "7  [<1-hop>\\n\\nInclusion of Clinical Work in a St...   \n",
              "8  [<1-hop>\\n\\nnon-term (includes clock-hour cale...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  For more detail on subscription-based programs...   \n",
              "1  Clinical work in nursing programs may be inclu...   \n",
              "2  Title IV program disbursements, except for the...   \n",
              "3  The requirements for clinical or practicum exp...   \n",
              "4  If a program has self-paced or independent stu...   \n",
              "5  In clock-hour or non-term credit-hour programs...   \n",
              "6  In subscription-based programs, the administra...   \n",
              "7  When a medical program includes clinical work ...   \n",
              "8  For students in non-term or subscription-based...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  multi_hop_abstract_query_synthesizer  \n",
              "4  multi_hop_abstract_query_synthesizer  \n",
              "5  multi_hop_abstract_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "ragas_dataset.to_pandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Retrievals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ragas.testset.synthesizers.testset_schema.Testset"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(ragas_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating naive retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab583a40087f4c93b3c4dd6e75aad8c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Questions progress:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating bm25 retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b815923a8994e138b97770b9320f95f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Questions progress:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating contextual_compression retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63cdb3fee9324c5dafe09e335971e275",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Questions progress:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating parent_document retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5644bb92a814d37805704e173ff38ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Questions progress:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ensemble retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69e7256288cc4d44bff6900e19b58514",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Questions progress:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "\n",
        "retrieval_strategies = {\n",
        "    'naive': naive_retrieval_chain, \n",
        "    'bm25': bm25_retrieval_chain, \n",
        "    'contextual_compression': contextual_compression_retrieval_chain,\n",
        "    'parent_document': parent_document_retrieval_chain,\n",
        "    'ensemble': ensemble_retrieval_chain\n",
        "    }\n",
        "\n",
        "dataset_with_response = {}\n",
        "for retrieval, chain in retrieval_strategies.items():\n",
        "    print(f\"Evaluating {retrieval} retrieval strategy\")\n",
        "    for test_row in tqdm_notebook(ragas_dataset, desc=\"Questions progress\"):\n",
        "        response = chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "        test_row.eval_sample.response = response[\"response\"].content\n",
        "        test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "    dataset_with_response[retrieval] = ragas_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['naive', 'bm25', 'contextual_compression', 'parent_document', 'ensemble'])"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_with_response.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'To learn more about subscription-based programs in Volume 2, Chapter 2, I do not have specific information from the provided context. You might consider checking the table of contents, index, or chapter headings in Volume 2, Chapter 2 of the relevant material. Alternatively, consult the book or document directly for references to subscription-based programs within that chapter. If you have access to a search function, try searching for key terms like \"subscription\" or \"subscription-based programs\" in that chapter. If you\\'re referring to a specific textbook, report, or manual, please specify its title so I can assist further.'"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_with_response['naive'].samples[0].eval_sample.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating naive retrieval strategy\n",
            "Evaluating bm25 retrieval strategy\n",
            "Evaluating contextual_compression retrieval strategy\n",
            "Evaluating parent_document retrieval strategy\n",
            "Evaluating ensemble retrieval strategy\n"
          ]
        }
      ],
      "source": [
        "from ragas import EvaluationDataset, evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\", max_tokens= 32768))\n",
        "\n",
        "evaluation_dataset = {}\n",
        "for retrieval, dataset in dataset_with_response.items():\n",
        "    print(f\"Evaluating {retrieval} retrieval strategy\")\n",
        "    evaluation_dataset[retrieval] = EvaluationDataset.from_pandas(dataset.to_pandas())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating naive retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e6466afd86241898e2c70456e4971d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[4]: TimeoutError()\n",
            "Exception raised in Job[10]: TimeoutError()\n",
            "Exception raised in Job[16]: TimeoutError()\n",
            "Exception raised in Job[22]: TimeoutError()\n",
            "Exception raised in Job[28]: TimeoutError()\n",
            "Exception raised in Job[34]: TimeoutError()\n",
            "Exception raised in Job[40]: TimeoutError()\n",
            "Exception raised in Job[46]: TimeoutError()\n",
            "Exception raised in Job[52]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating bm25 retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a1ec9add4904b1c8e7aa3e75cd8ec6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[41]: ValueError(setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (15,) + inhomogeneous part.)\n",
            "Exception raised in Job[4]: TimeoutError()\n",
            "Exception raised in Job[10]: TimeoutError()\n",
            "Exception raised in Job[16]: TimeoutError()\n",
            "Exception raised in Job[22]: TimeoutError()\n",
            "Exception raised in Job[28]: TimeoutError()\n",
            "Exception raised in Job[34]: TimeoutError()\n",
            "Exception raised in Job[40]: TimeoutError()\n",
            "Exception raised in Job[46]: TimeoutError()\n",
            "Exception raised in Job[52]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating contextual_compression retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa631da0426a49cf872411a7b735d364",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[4]: TimeoutError()\n",
            "Exception raised in Job[10]: TimeoutError()\n",
            "Exception raised in Job[16]: TimeoutError()\n",
            "Exception raised in Job[22]: TimeoutError()\n",
            "Exception raised in Job[28]: TimeoutError()\n",
            "Exception raised in Job[34]: TimeoutError()\n",
            "Exception raised in Job[40]: TimeoutError()\n",
            "Exception raised in Job[46]: TimeoutError()\n",
            "Exception raised in Job[52]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating parent_document retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec3664890eea42a38bea04ecd27d0d52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[4]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[10]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[28]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[22]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[34]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[40]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[46]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[52]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ensemble retrieval strategy\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d126b0c0828f4f098733fcc6e27aba49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[37]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-nano in organization org-URUSKKyHzl4ZMdhZ2HYDH466 on tokens per min (TPM): Limit 200000, Used 200000, Requested 8857. Please try again in 2.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[22]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[34]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[28]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[40]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[46]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[52]: TimeoutError()\n"
          ]
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=600)\n",
        "\n",
        "evaluation_results = {}\n",
        "for retrieval, dataset in evaluation_dataset.items():\n",
        "    print(f\"Evaluating {retrieval} retrieval strategy\")\n",
        "    evaluation_results[retrieval] = evaluate(\n",
        "        dataset=dataset,\n",
        "        metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "        llm=evaluator_llm,\n",
        "        run_config=custom_run_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "naive\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 1.0000, 'faithfulness': 0.8833, 'factual_correctness(mode=f1)': 0.8211, 'answer_relevancy': 0.6161, 'context_entity_recall': nan, 'noise_sensitivity(mode=relevant)': 0.0106}"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('naive')\n",
        "evaluation_results['naive']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bm25\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 1.0000, 'faithfulness': 0.7778, 'factual_correctness(mode=f1)': 0.7911, 'answer_relevancy': 0.7154, 'context_entity_recall': nan, 'noise_sensitivity(mode=relevant)': 0.0326}"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('bm25')\n",
        "evaluation_results['bm25']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context_recall': 1.0000, 'faithfulness': 0.9504, 'factual_correctness(mode=f1)': 0.7689, 'answer_relevancy': 0.7118, 'context_entity_recall': nan, 'noise_sensitivity(mode=relevant)': 0.0344}"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('contextual_compression')\n",
        "evaluation_results['contextual_compression']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "parent_document\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 1.0000, 'faithfulness': 0.7778, 'factual_correctness(mode=f1)': 0.7978, 'answer_relevancy': 0.7164, 'context_entity_recall': 0.0000, 'noise_sensitivity(mode=relevant)': 0.0339}"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('parent_document')\n",
        "evaluation_results['parent_document']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ensemble\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 1.0000, 'faithfulness': 0.8750, 'factual_correctness(mode=f1)': 0.8811, 'answer_relevancy': 0.6154, 'context_entity_recall': 0.0000, 'noise_sensitivity(mode=relevant)': 0.0487}"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('ensemble')\n",
        "evaluation_results['ensemble']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langsmith evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - SDG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"Assignment 9 - Loan Synthetic Data\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Assignment 9 - Loan Synthetic Data\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data_row in ragas_dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"response\"],\n",
        "        \"reference\": example.outputs[\"answer\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating naive retrieval strategy\n",
            "View the evaluation results for experiment: 'giving-straw-29' at:\n",
            "https://smith.langchain.com/o/6ce7a019-3815-4374-be47-38dfdf42ee54/datasets/3a9bf070-f6b1-46d8-9652-fd3405e96087/compare?selectedSessions=7d4719be-0e8c-49b6-acec-d00af7776a23\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fabcc7a45124627b0f6c07de2ca4dfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating bm25 retrieval strategy\n",
            "View the evaluation results for experiment: 'worthwhile-record-90' at:\n",
            "https://smith.langchain.com/o/6ce7a019-3815-4374-be47-38dfdf42ee54/datasets/3a9bf070-f6b1-46d8-9652-fd3405e96087/compare?selectedSessions=7c79a3fd-c493-4b6e-9f80-41e987b4297c\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9591c3d215814b869ea1fe2bdaa08ab6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating contextual_compression retrieval strategy\n",
            "View the evaluation results for experiment: 'elderly-part-44' at:\n",
            "https://smith.langchain.com/o/6ce7a019-3815-4374-be47-38dfdf42ee54/datasets/3a9bf070-f6b1-46d8-9652-fd3405e96087/compare?selectedSessions=0ad724a7-777a-4f88-9bdd-53e21a808c65\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "212a4242d759469f9f791a7c8398168b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating parent_document retrieval strategy\n",
            "View the evaluation results for experiment: 'upbeat-stove-67' at:\n",
            "https://smith.langchain.com/o/6ce7a019-3815-4374-be47-38dfdf42ee54/datasets/3a9bf070-f6b1-46d8-9652-fd3405e96087/compare?selectedSessions=dc376ca5-4827-42f1-8464-2ca7f20bfee9\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "492aff353552459eb97d76f83f16521a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ensemble retrieval strategy\n",
            "View the evaluation results for experiment: 'spotless-cod-35' at:\n",
            "https://smith.langchain.com/o/6ce7a019-3815-4374-be47-38dfdf42ee54/datasets/3a9bf070-f6b1-46d8-9652-fd3405e96087/compare?selectedSessions=bb354749-4ce2-4837-bb1f-35a13a72bef5\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2b8d9e2c2714c6f85937ebeb59feef7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "retrieval_strategies_langsmith = {\n",
        "    'naive': naive_retrieval_chain_langsmith, \n",
        "    'bm25': bm25_retrieval_chain_langsmith, \n",
        "    'contextual_compression': contextual_compression_retrieval_chain_langsmith,\n",
        "    'parent_document': parent_document_retrieval_chain_langsmith,\n",
        "    'ensemble': ensemble_retrieval_chain_langsmith\n",
        "    }\n",
        "\n",
        "results = {}\n",
        "for retrieval, chain in retrieval_strategies_langsmith.items():\n",
        "    print(f\"Evaluating {retrieval} retrieval strategy\")\n",
        "    evaluate(\n",
        "        chain.invoke,\n",
        "        data=dataset_name,\n",
        "        evaluators=[\n",
        "            qa_evaluator,\n",
        "            labeled_helpfulness_evaluator\n",
        "        ],\n",
        "        metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAGAS Evaluation Results Summary\n",
        "\n",
        "## Performance Metrics Comparison\n",
        "\n",
        "| Strategy | Context Recall | Faithfulness | Factual Correctness | Answer Relevancy | Context Entity Recall | Noise Sensitivity | Latency (s) | Cost ($) | Tokens |\n",
        "|----------|----------------|--------------|---------------------|------------------|----------------------|-------------------|-------------|----------|---------|\n",
        "| **Naive** | 1.0000 | 0.8833 | 0.8211 | 0.6161 | NaN | 0.0106 | 4.321 | 0.082 | 71,926 |\n",
        "| **BM25** | 1.0000 | 0.7778 | 0.7911 | 0.7154 | NaN | 0.0326 | 2.422 | 0.004 | 31,363 |\n",
        "| **Contextual Compression** | 1.0000 | 0.9504 | 0.7689 | 0.7118 | NaN | 0.0344 | 3.165 | 0.0034 | 24,503 |\n",
        "| **Parent Document** | 1.0000 | 0.7778 | 0.7978 | 0.7164 | 0.0000 | 0.0339 | 3.575 | 0.0048 | 39,079 |\n",
        "| **Ensemble** | 1.0000 | 0.8750 | 0.8811 | 0.6154 | 0.0000 | 0.0487 | 7.791 | 0.0189 | 175,337 |\n",
        "\n",
        "## Metric Definitions\n",
        "\n",
        "- **Context Recall**: Measures how well the retrieved context covers the information needed to answer the question (higher is better)\n",
        "- **Faithfulness**: Evaluates whether the generated answer is faithful to the retrieved context (higher is better)\n",
        "- **Factual Correctness**: Measures the factual accuracy of the generated answers (higher is better)\n",
        "- **Answer Relevancy**: Assesses how relevant the generated answer is to the original question (higher is better)\n",
        "- **Context Entity Recall**: Measures entity-level recall from the retrieved context (higher is better)\n",
        "- **Noise Sensitivity**: Evaluates robustness to irrelevant information in context (lower is better)\n",
        "\n",
        "## Analysis: Optimal Strategy Selection\n",
        "\n",
        "**BM25 emerges as the optimal strategy** for this financial aid documentation dataset, providing the best balance of performance, cost, and efficiency.\n",
        "\n",
        "**Performance Advantages**: BM25 achieves the highest answer relevancy (0.7154) among all strategies, indicating it generates the most relevant responses to user questions. While it has slightly lower faithfulness (0.7778) compared to contextual compression, it maintains strong factual correctness (0.7911) and perfect context recall (1.0000).\n",
        "\n",
        "**Cost Efficiency**: BM25 is the most cost-effective option at $0.004, using only 31,363 tokens. This represents a 95% cost reduction compared to the naive approach ($0.082) while maintaining competitive performance metrics.\n",
        "\n",
        "**Latency Performance**: At 2.422 seconds, BM25 provides the fastest response time among all strategies, making it suitable for real-time applications where speed is crucial.\n",
        "\n",
        "**Strategic Trade-offs**: While contextual compression achieves the highest faithfulness (0.9504), it comes with higher latency (3.165s) and slightly lower answer relevancy. The ensemble approach, despite having the highest factual correctness (0.8811), is prohibitively expensive ($0.0189) and slow (7.791s), making it impractical for production use.\n",
        "\n",
        "For this financial aid documentation dataset, BM25's keyword-based retrieval approach effectively captures the specific terminology and concepts users are likely to query, while its computational efficiency makes it suitable for deployment in cost-sensitive environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
